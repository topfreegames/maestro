{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>Maestro is a game room orchestration system, which can be used by multiplayer games to manage the scalability of its game rooms (Aka game servers). Ideally, Maestro should be used by games that implement dedicated game server (dgs) architecture. Each game room is a dedicated game server that runs in a match execution context, a group of game rooms is organized in a Scheduler. Each time that a user requires some change in the Scheduler or game room, Maestro creates an Operation that is a change unit that will be enqueued and handled sequentially by a proper worker related exclusively to its respective Scheduler, in other words, each Scheduler has a worker that sequentially handles Operations that is created to it.</p> <p>A Scheduler defines the requirements of a game room as to how much memory and CPU it will need to execute and other information.</p> <p>Each Operation has its own properties to be executed.</p> <p>Maestro has five components: Management API, Game Rooms API, Execution Worker, Runtime Watcher and Metrics Reporter. They all deliver together the Maestro features that will be described in the next sections.</p>"},{"location":"#features-components","title":"Features &amp; Components","text":"<p>Maestro is composed by:</p> <ul> <li>Management API: this is the component that the users will use to create your requests to interact with Maestro. For example: create a Scheduler, get Scheduler information, etc.</li> <li>Execution Worker: have an execution component to handle Operations to each Scheduler. For example: three Schedulers will have each one an execution component.</li> <li>Game Rooms API: game rooms API exposes an HTTP API or a GRPc service to receive game rooms messages that symbolize their respective status. For example: when a game room is ready to receive matches it will send a message informing that.</li> <li>Runtime Watcher: is a component in Maestro that listen to Runtime events and reflect them in Maestro. For example: when a game room was created it is notified that this event happened.</li> <li>Metrics Reporter: time spaced this component query the Runtime for metrics related to rooms and expose them in an open metrics route. For example: how many occupied rooms and ready rooms are up.</li> </ul>"},{"location":"reference/","title":"Reference","text":"<p>The reference documentation are available for this module:</p> <ul> <li> <p>Maestro Architecture</p> </li> <li> <p>Open API</p> </li> <li> <p>Schedulers</p> </li> <li> <p>Operations</p> </li> <li> <p>Rolling Update</p> </li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":"<p>The following tutorials are available for this module:</p> <ul> <li> <p>Getting Started Guide</p> </li> <li> <p>Configuring Scheduler Autoscaling</p> </li> <li> <p>Configuring Events Forwarding</p> </li> <li> <p>Development</p> </li> </ul>"},{"location":"how-to-guides/Guidelines/","title":"Guidelines","text":""},{"location":"how-to-guides/Guidelines/#assumptions-dos-donts","title":"Assumptions, Dos &amp; Don'ts","text":"<p>Describe common assumptions, or actions that you want your users to avoid. If there are multiple ways to solve problems using your application, you can document here the best options you have identified.</p>"},{"location":"how-to-guides/Guidelines/#lessons-learned-best-practices","title":"Lessons Learned &amp; Best Practices","text":"<p>You can fill this section with best practices and enrich it as you learn better practices for the use of your application.</p>"},{"location":"how-to-guides/User%20Guides/","title":"UserGuides","text":""},{"location":"how-to-guides/User%20Guides/#features-components","title":"Features &amp; Components","text":"<p>Short list of the components, features or elements that the user needs to know to make the best of your application. On this section, you can include any guides and how tos that involve interaction with the UI in RING.</p>"},{"location":"how-to-guides/User%20Guides/#navigation-how-to-guides","title":"Navigation &amp; How To Guides","text":"<p>Describe any menus and buttons in the user interface not covered in the previous section Include how users can interact with the features on your interface to get the desired results.</p>"},{"location":"how-to-guides/User%20Guides/#troubleshooting","title":"Troubleshooting","text":"<p>Document here any known errors, workarounds for bugs, troubleshooting details, and tips and tricks to avoid common mistakes.</p>"},{"location":"reference/Architecture/","title":"Architecture","text":"<p>Maestro Next is a composition of different modules. Internally they are all part of the same code base but could be executed by giving the right arguments to the command line (or to your docker container entry point). E.g. <code>go run main.go start [MODULE_NAME]</code></p> <p>Maestro is composed of Management API, Rooms API, Operation Execution Worker, Runtime Watcher Worker, and Metrics Reporter Worker. </p> <p>Each module has its responsibilities and is divided apart in a way to avoid mixing the execution process. Each module was thought to avoid parallel problems and to give the client more visibility about which Operations are being executed and their respective status.</p> <p></p>"},{"location":"reference/Architecture/#maestro-modules","title":"Maestro modules","text":"<p>Note: Maestro currently only supports Kubernetes as Game Rooms runtime system. So Workers interact with them.</p>"},{"location":"reference/Architecture/#management-api","title":"Management API","text":"<p>Management API is the module responsible for receiving user requests. It accepts gRPC and HTTP requests and provides several kinds of routes that are aggregated in two services: schedulers service and operations service.</p> <p>The schedulers service exposes features for managing schedulers, like creating a new scheduler, fetching its information, or updating them. The operations service exposes features for tracking operations and changing their status, like listing operations by status or canceling them.</p> <p>Management API relies on Redis for retrieving operations and game rooms, and on Postgres for retrieving and persisting schedulers.</p> <p></p>"},{"location":"reference/Architecture/#rooms-api","title":"Rooms API","text":"<p>Rooms API is the module that provides an API that must be used by game rooms to sync their status with Maestro. To maestro work properly, it needs to be constantly informed about the status of each game room it manages. Also, if there are forwarders configured for the scheduler, those events are forwarded from Maestro at this module.</p> <p>Note: The requests that Maestro forwards in the Rooms API are documented in this proto file. Note: Maestro client could be used to ease the integration of the Game Room with Maestro.</p> <p></p>"},{"location":"reference/Architecture/#operation-execution-worker","title":"Operation Execution Worker","text":"<p>Note: In Maestro a worker is a collection of routines that executes a flow related to one and only one Scheduler each.</p> <p>Operation Execution Worker is a process that constantly keeps ensuring each active Scheduler will have a thread (execution worker) that executes operations enqueued in the related Scheduler operation queue. So in this way became possible to track the events that happened and change a certain Scheduler in a healthier way.</p> <p>You could find all operations at Operations section</p> <p></p>"},{"location":"reference/Architecture/#runtime-watcher-worker","title":"Runtime Watcher Worker","text":"<p>Note: In Maestro a worker is a collection of routines that executes a flow related to one and only one Scheduler each.</p> <p>Runtime Watcher Worker listens to runtime events related to the Scheduler and reflects the changes in Maestro. Currently, it mitigate disruptions by looking at the current amount of occupied rooms, and it listens for Game Rooms creation, deletion, and update.</p> <p></p>"},{"location":"reference/Architecture/#metrics-reporter-worker","title":"Metrics Reporter Worker","text":"<p>Note: In Maestro a worker is a collection of routines that executes a flow related to one and only one Scheduler each.</p> <p>From time to time Metrics Reporter Worker watch runtime to report metrics from them, such as the number of game rooms instances that are <code>ready</code>, <code>pending</code>, <code>error</code>, <code>unknown</code>, or <code>terminating</code> status. As well it watches from Game Rooms storage its status that could be <code>ready</code>, <code>pending</code>, <code>error</code>, <code>occupied</code>, <code>terminating</code>, or <code>unready</code>.</p> <p>This module is optional since you don't need it for any specific functionalities of the application.</p> <p></p>"},{"location":"reference/Kubernetes/","title":"Kubernetes Usage","text":""},{"location":"reference/Kubernetes/#kubernetes-usage","title":"Kubernetes usage","text":"<p>Maestro uses kubernetes for orchestrating game room instances. It uses a unique namespace for each scheduler, and a unique pod for each game room instance.</p> <p>We use client-go for communicating with kubernetes. The Runtime port is the interface used for managing resources, you can find all of the features we are using for managing k8s resources in it.</p> <p>The diagram below shows how maestro components interact with kubernetes for managing resources.</p> <pre><code>flowchart BT\n  classDef borderless stroke-width:0px\n  classDef darkBlue fill:#00008B, color:#fff\n  classDef brightBlue fill:#6082B6, color:#fff\n  classDef gray fill:#62524F, color:#fff\n  classDef gray2 fill:#4F625B, color:#fff\n  subgraph maestroSystem[ ]\n      subgraph k8s[ ]\n          A3[Kubernetes]\n      end\n      class k8s,A3 brightBlue\n      class A3, borderless\n      subgraph WORKER[ ]\n          A7[Operation Execution Worker&lt;br/&gt;&lt;br/&gt;manage kubernetes resources by creating/deleting/updating pods abnd namespaces]\n      end\n      class WORKER,A7 brightBlue\n      class WORKER,A7 borderless\n      WORKER--Create namespace&lt;br/&gt;HTTPS--&gt;k8s\n      WORKER--Delete namespace&lt;br/&gt;HTTPS--&gt;k8s\n      WORKER--Create pod&lt;br/&gt;HTTPS--&gt;k8s\n      WORKER--Delete pod&lt;br/&gt;HTTPS--&gt;k8s\n      subgraph RUNTIME_WATCHER[ ]\n          A8[Runtime Watcher &lt;br/&gt;&lt;br/&gt; watch for change events in managed pods]\n      end\n      class RUNTIME_WATCHER,A8 brightBlue\n      class RUNTIME_WATCHER,A8 borderless\n      RUNTIME_WATCHER--List/Watch pods&lt;br/&gt;HTTPS--&gt;k8s\n  end\n  click A3 \"/csymapp/mermaid-c4-model/blob/master/AWAComponent.md\" \"AWA\"\n</code></pre>"},{"location":"reference/Kubernetes/#runtime-watcher","title":"Runtime watcher","text":"<p>The runtime watcher component spawn two types of workers: one that is responsible for mitigating disruptions and another for processing change events in pods resources.</p>"},{"location":"reference/Kubernetes/#disruption-worker","title":"Disruption Worker","text":"<p>This worker consists in a single goroutine with a ticker. Each time it runs, it will check the number of occupied rooms at the time and try to mitigate disruptions. For k8s, this mitigation consists on applying a PDB to the scheduler's namespace, that has <code>minAvailable</code> equals to the number of occupied rooms plus a safety percentage.</p> <p>One can configure the interval in which this worker runs and also the safety percentage in <code>config/config.yaml</code>:</p> <pre><code>runtimeWatcher:\n  disruptionWorker:\n    intervalSeconds: 5\n    safetyPercentage: 0.05\n</code></pre> <p>We should always allow node consolidation to happen, thus if the total number of rooms is 1, then we set PDB's <code>minAvailable: 0</code>.</p>"},{"location":"reference/Kubernetes/#pod-change-events-worker","title":"Pod Change Events Worker","text":"<p>For this type of worker, runtime watcher spawns multiple goroutines, maintaining a worker process for each scheduler that keeps watching and processing change events in pods resources. For doing that, it uses a pods informer, binding handlers for add, update and delete events for all pods managed by it.</p> <p>This component is not responsible for updating/creating/deleting kubernetes resources, all it does is to watch for changes and update its game room instances internal representation using redis.</p>"},{"location":"reference/Kubernetes/#operation-execution-worker","title":"Operation execution worker","text":"<p>The worker uses kubernetes for managing pods and namespaces. It executes several operations that, alongside other side effects, will need to create, update, and delete namespaces and pods.</p> <p>Currently, maestro does not check for HostPort conflict while creating new rooms</p> <p>One important note regarding how maestro creates pods: each new requested game room instance will be assigned to a pseudo-random port to be used as HostPort.</p> <p>Maestro uses the scheduler PortRange to generate the pseudo-random port. Currently, maestro does not check for HostPort conflict while creating new rooms. The final address of the game room will be composed of the Node address and the game room container assigned HostPort. That's the reason why maestro needs access for reading the Node addresses.</p>"},{"location":"reference/Kubernetes/#configuring-cluster-access","title":"Configuring cluster access","text":"<p>Maestro needs the following permissions for managing resources in a kubernetes cluster: - nodes: read (we need to use the node address to compose the game room address); - pods: read, create, update, delete; - namespace: read, create, update, delete.</p> <p>Maestro provides two ways for configuring kubernetes cluster access.</p>"},{"location":"reference/Kubernetes/#using-incluster-mode","title":"Using inCluster mode","text":"<p>Set <code>adapters.runtime.kubernetes.inCluster</code> config value to true or use its env var equivalent, the kubernetes client will be configured automatically using the same service account of the maestro component running pod.</p> <p>The kubernetes client has a default rate limiter which implements a token bucket approach. The inCluster mode allows rate limit changes via <code>adapters.runtime.kubernetes.qps</code> and <code>adapters.runtime.kubernetes.burst</code> configs.</p> <p>This mode is recommended to be used when running maestro components in the same cluster in which the schedulers and rooms will be managed.</p>"},{"location":"reference/Kubernetes/#using-kubeconfig-mode","title":"Using kubeconfig mode","text":"<p>Populate <code>adapters.runtime.kubernetes.kubeconfig</code> and <code>adapters.runtime.kubernetes.masterUrl</code> configs or use its env var equivalent, the kubernetes client will be configured using the provided kubeconfig file and master url.</p>"},{"location":"reference/OpenAPI/","title":"OpenAPI","text":""},{"location":"reference/Operations/","title":"Operations","text":""},{"location":"reference/Operations/#what-is","title":"What is","text":"<p>Operation is a core concept at Maestro, and it represents executions done in multiple layers of Maestro, a state update, or a configuration change. Operations can be created by user actions while managing schedulers (e.g. consuming management API), or internally by Maestro to fulfill internal states requirements</p> <p>Operations are heavily inspired by the Command Design Pattern</p>"},{"location":"reference/Operations/#definition-and-executors","title":"Definition and executors","text":"<p>Maestro will have multiple operations, and those will be set using pairs of definitions and executors. An operation definition consists of the operation parameters. An operation executor is where the actual operation execution and rollback logic is implemented, and it will receive as input its correlated definition. So, for example, the <code>CreateSchedulerExecutor</code> will always receive a <code>CreateSchedulerDefinition</code>.</p> <pre><code>flowchart TD\n  subgraph operations [Operations]\n    subgraph operation_implementation [Operation Impl.]\n      definition(Definition)\n      executor(Executor)\n    end\n    subgraph operation_implementation2 [Operation Impl.]\n      definition2(Definition)\n      executor2(Executor)\n    end\n    subgraph operation_implementation3 [Operation Impl.]\n      definition3(Definition)\n      executor3(Executor)\n    end\n    ...\n  end\n</code></pre>"},{"location":"reference/Operations/#operation-structure","title":"Operation Structure","text":"<ul> <li>id: Unique operation identification. Auto-Generated;</li> <li>status: Operations status. For reference, see here.</li> <li>definitionName: Name of the operation. For reference, see here.</li> <li>schedulerName: Name of the scheduler which this operation affects.</li> <li>createdAt: Timestamp representing when the operation was enqueued.</li> <li>input: Contains the input value for this operation. Each operation has its own input format.   For details, see below.</li> <li>executionHistory: Contains logs with detailed info about the operation execution. See below.</li> </ul> <pre><code>id: String\nstatus: String\ndefinitionName: String\nschedulerName: String\ncreatedAt: Timestamp\ninput: Any\nexecutionHistory: ExecutionHistory\n</code></pre>"},{"location":"reference/Operations/#input","title":"Input","text":"<ul> <li>Create Scheduler</li> </ul> <pre><code>scheduler: Scheduler\n</code></pre> <ul> <li>Create New Scheduler Version</li> </ul> <pre><code>scheduler: Scheduler\n</code></pre> <ul> <li>Switch Scheduler Version</li> </ul> <pre><code>newActiveVersion: Scheduler\n</code></pre> <ul> <li>Add Rooms</li> </ul> <pre><code>amount: Integer\n</code></pre> <ul> <li>Remove Rooms</li> </ul> <pre><code>amount: Integer\n</code></pre>"},{"location":"reference/Operations/#execution-history","title":"Execution History","text":"<pre><code>createdAt: timestamp\nevent: String\n</code></pre> <ul> <li>createdAt: When did the event happened.</li> <li>event: What happened. E.g. \"Operation failed because...\".</li> </ul>"},{"location":"reference/Operations/#how-does-maestro-handle-operations","title":"How does Maestro handle operations","text":"<ul> <li>Each scheduler has 1 operation execution (no operations running in parallel for a scheduler).</li> <li>Every operation execution has 1 queue for pending operations.</li> <li>When the worker is ready to work on a new operation, it'll pop from the queue.</li> <li>The operation is executed by the worker following the lifecycle described here.</li> </ul>"},{"location":"reference/Operations/#state","title":"State","text":"<p>An operation can have one of the Status below:</p> <ul> <li> <p>Pending: When an operation is enqueued to be executed;</p> </li> <li> <p>Evicted: When an operation is unknown or should not be executed By Maestro;</p> </li> <li> <p>In Progress: Operation is currently being executed;</p> </li> <li> <p>Finished: Operation finished; Execution succeeded;</p> </li> <li> <p>Error: Operation finished. Execution failed;</p> </li> <li> <p>Canceled: Operation was canceled by the user.</p> </li> </ul>"},{"location":"reference/Operations/#state-machine","title":"State Machine","text":"<pre><code>flowchart TD\n  pending(Pending)\n  in_progress(In Progress)\n  evicted(Evicted)\n  finished(Finished)\n  canceled(Canceled)\n  error(Error)\n\n  pending --&gt; in_progress;\n  pending --&gt; evicted;\n  in_progress --&gt; finished;\n  in_progress --&gt; error;\n  in_progress --&gt; canceled;\n</code></pre>"},{"location":"reference/Operations/#lifecycle","title":"Lifecycle","text":"<pre><code>flowchart TD\n  finish((End))\n  created(\"Created (Pending)\")\n  evicted(Evicted)\n  error(Error)\n  finished(Finished)\n  canceled(Canceled)\n  should_execute{Should Execute?}\n  execution_succeeded{Success?}\n  err_kind{Error Kind}\n  execute[[Execute]]\n  rollback[[Rollback]]\n  canceled_by_user&gt;Canceled By User]\n  created --&gt; should_execute;\n  should_execute -- No --&gt; evicted --&gt; finish;\n  should_execute -- Yes --&gt; execute;\n  execute --&gt; execution_succeeded;\n  execute --&gt; canceled_by_user --&gt; rollback;\n  execution_succeeded -- Yes --&gt; finished --&gt; finish;\n  execution_succeeded -- No --&gt; rollback;\n  rollback --&gt; err_kind;\n  err_kind -- Canceled --&gt; canceled --&gt; finish;\n  err_kind -- Error --&gt; error --&gt; finish\n</code></pre>"},{"location":"reference/Operations/#lease","title":"Lease","text":""},{"location":"reference/Operations/#what-is-the-operation-lease","title":"What is the operation lease","text":"<p>Lease is a mechanism to track the operations' execution process and check if we can rely on the current/future operation state. </p>"},{"location":"reference/Operations/#why-operations-have-it","title":"Why Operations have it","text":"<p>Sometimes, an operation might get stuck. It could happen, for example, if the worker crashes during the execution of an operation. To keep track of operations, we assign each operation a Lease. This Lease has a TTL (time to live). </p> <p>When the operation is being executed, this TTL is renewed each time the lease is about to expire while the operation is still in progress. It'll be revoked once the operation is finished.</p>"},{"location":"reference/Operations/#troubleshooting","title":"Troubleshooting","text":"<p>If an operation is fetched and the TTL expired (the TTL is in the past), the operation probably got stuck,  and we can't rely upon its current state, nor guarantee the required side effects of the execution or rollback have succeeded.</p> <p>If an operation does not have a Lease, it either did not start at all (should be on the queue) or is already finished.  An Active Operation without a Lease is at an invalid state.</p> <p>Maestro do not have a self-healing routine yet for expired operations.</p>"},{"location":"reference/Operations/#operation-lease-lifecycle","title":"Operation Lease Lifecycle","text":"<pre><code>flowchart TD\n  finish_routine((End))\n  start_routine((Start))\n  finish((End))\n\n  operation_finished{Op. Finished?}\n\n  to_execute[Operation to Execute]\n\n  grant_lease[[Grant Lease]]\n  renew_lease[[Renew Lease]]\n  revoke_lease[[Revoke Lease]]\n\n  wait_for_ttl(Wait for TTL)\n  execute(Execute Operation)\n\n  to_execute --&gt; grant_lease;\n  grant_lease --&gt; renew_lease_routine;\n  grant_lease --&gt; execute;\n  subgraph renew_lease_routine [ASYNC Renew Lease Routine]\n      start_routine --&gt; wait_for_ttl;\n      wait_for_ttl --&gt; operation_finished;\n      operation_finished -- Yes --&gt; revoke_lease;\n      operation_finished -- No --&gt; renew_lease --&gt; wait_for_ttl;\n      revoke_lease --&gt; finish_routine;\n  end\n  renew_lease_routine --&gt; finish;\n</code></pre>"},{"location":"reference/Operations/#available-operations","title":"Available Operations","text":"<p>For more details on how to use Maestro API, see this section.</p>"},{"location":"reference/Operations/#create-scheduler","title":"Create Scheduler","text":"<ul> <li>Accessed through the <code>POST /schedulers</code> endpoint.</li> <li>Creates the scheduler structure for receiving rooms;</li> <li>The scheduler structure is validated, but the game room is not;</li> <li>If operation fails, rollback feature will delete anything created related to scheduler.</li> </ul>"},{"location":"reference/Operations/#create-new-scheduler-version","title":"Create New Scheduler Version","text":"<ul> <li>Accessed through the <code>POST /schedulers/:schedulerName</code> endpoint.</li> <li>Creates a validation room (deleted right after).     If Maestro cannot receive pings (not forwarded) from validation game room, operation fails;</li> <li>When this operation finishes successfully, it enqueues the \"Switch Active Version\".</li> <li>If operation fails rollback routine deletes anything (except for the operation) created related to new version.</li> </ul>"},{"location":"reference/Operations/#switch-active-version","title":"Switch Active Version","text":"<ul> <li>Accessed through <code>PUT /schedulers/:schedulerName</code> endpoint.</li> <li>If it's a major change (anything under Scheduler.Spec changed), GRUs are replaced using scheduler maxSurge property;</li> <li>If it's a minor change (Scheduler.Spec haven't changed), GRUs are not replaced;</li> </ul>"},{"location":"reference/Operations/#add-rooms","title":"Add Rooms","text":"<ul> <li>Accessed through <code>POST /schedulers/:schedulerName/add-rooms</code> endpoint.</li> <li>If any room fail on creating, the operation fails and created rooms are deleted on rollback feature;</li> </ul>"},{"location":"reference/Operations/#remove-rooms","title":"Remove Rooms","text":"<ul> <li>Accessed through <code>POST /schedulers/:schedulerName/remove-rooms</code> endpoint.</li> <li>Remove rooms based on amount;</li> <li>Rollback routine does nothing.</li> </ul>"},{"location":"reference/RollingUpdate/","title":"Rolling Update","text":"<p>When the scheduler is updated, only minor, the switch version operation will simply change the new active version in the database. Thus, who is responsible for actually performing runtime changes to enforce that all existing Game Rooms are from the active schedulers is the health_controller operation.</p> <p>This has some benefits attached to it:</p> <ol> <li>Consolidates all game room operations in the Add and Remove operations</li> <li>If the update fails, rollback is more smooth. Previously, in the switch_version opeartion, if any creation/deletion of GRs failed, we would sequentially delete all created game rooms. This can hog the worker in this operation (no upscale during this time) and also the delete was not taking into consideration ready target, thus it could heavily offend it</li> <li>Each health_controller loop is able to adjust how much game rooms it asks for creation and process operations in between, avoiding the worker to be hogged in a single operation</li> </ol>"},{"location":"reference/RollingUpdate/#add-rooms-limit-impacts","title":"Add Rooms Limit Impacts","text":"<p>Keep in mind that the add_rooms operation has its own limit of rooms that can create per operation, thus if more rooms than the limit is requested Maestro will cap to this limit and on the next cycle a new set of rooms will be requested.</p> <p>For example, if on the first cycle the autoscale asks for 1000 rooms and limit is set to 150, then only 150 are created and in the next cycle 850 rooms (assuming the autoscale compute is the same) will be requested - until the desired number of rooms is reached.</p>"},{"location":"reference/RollingUpdate/#old-behavior","title":"Old behavior","text":"<ol> <li>A new scheduler version is created or a call to switch the active version is made</li> <li>new_version operation starts by the worker</li> <li>A new validation room is created to validate the scheduler config. If this room becomes ready, the new version is marked as safe to be deployed</li> <li>switch_version operation starts by the worker, other operations will wait until this one finishes (no autoscaling)</li> <li>Maestro spawns a <code>maxSurge</code> amount of goroutines reading from a channel. Each goroutine creates a game room with the new config and deletes a game room from previous config</li> <li>When all goroutines finish processing, meaning that there aren't any more GRs from the old version to be deleted (channel closed), the operation changes the active version in the database</li> <li>Operation switch_version finishes and other operations can run</li> </ol> <p>If at any point there is an error in one of the goroutines or other part of the process, worker starts to rollback. The rollback mechanism will delete all newly created game rooms sequentially, even if this means that the system will have no available game rooms by the end of it.</p>"},{"location":"reference/RollingUpdate/#new-behavior","title":"New behavior","text":"<p>The new behavior is heavily inspired by how Kubernetes perform rolling updates in deployments working gracefully alongside HPA (Horizontal Pod Autoscaler). Thus, two parameters will guide this process:</p> <ul> <li><code>maxUnavailable</code>: how many ready pods below the desired can exist at a time in the deployment. In other words, how many game rooms below <code>readyTarget</code> policy we can delete. For now, Maestro will use <code>maxUnavailable: 0</code>, which means we never go below the <code>readyTarget</code> at any given cycle of the update. This can make the process a bit slower, but it ensures higher availability.</li> <li> <p><code>maxSurge</code>: how many Game Rooms can be created at once (per cycle) and how many Game Rooms can exist above the desired. The later concept is important to understand that we should go above the desired, otherwise we can not spawn a new Game Room since it would offend the autoscaler/HPA that would try to delete it</p> </li> <li> <p>A new scheduler version is created or a call to switch the active version is made</p> </li> <li>new_version operation starts by the worker</li> <li>A new validation room is created to validate the scheduler config. If this room becomes ready, the new version is marked as safe to be deployed</li> <li>switch_version operation starts by the worker, simply changing the active version in the database</li> <li>health_controller operation runs and check if there is any existing game room that is not from the active scheduler version. More details on how we check for it below.</li> <li>Get all rooms from that scheduler</li> <li>For each room check if the version is not equal to the current active scheduler version</li> <li>Check if we have compared with this version before. If not, get the scheduler entity for that old version</li> <li>Compare the old version with the new one, if it's a major one add it to the comparison map</li> <li>At the end of the rooms loop, if we do not find any major change return false when checking for a rolling update. If we found a major change, append the occupied and then ready rooms to the array</li> <li>If it does not have, run normal autoscale. If it has, it is performing a rolling update, proceed with the update</li> <li>The update checks how many rooms it can spawn by computing the below:</li> <li>Calculate the current <code>deviation</code> (<code>totalRoomsAmount - desiredAmountOfRooms</code>);</li> <li>Calculate the <code>surge</code> amount over the desired (<code>surge = (maxSurge / 100) * desiredAmountOfRooms</code>);</li> <li>Discount the deviation from the surge (<code>surge = surge - deviation</code>);</li> <li>Enqueues a priority add_room operation to create the surge amount</li> <li>Check how many old rooms it can delete by computing</li> </ul> <pre><code>currentNumberOfReadyRooms - desiredAmountOfReadyRooms (autoscale)\n</code></pre> <ol> <li>If it can delete, enqueue a delete_room operation. The above is valid for <code>maxUnavailable: 0</code> so we never offend the <code>readyTarget</code>. Rooms are deleted by ID since Maestro must delete only the rooms that are not from the active scheduler version. Also, the occupied rooms will be the last one deleted from the list.</li> <li>One health_controller cycle finishes running the rolling update</li> <li>health_controller runs as many cycles as needed creating and deleting until there are no more rooms from non-active scheduler versions to be deleted. When this happens, rolling update finishes and health_controller performs normal autoscale</li> </ol>"},{"location":"reference/RollingUpdate/#scenarios","title":"Scenarios","text":"<p>Below you will find how rolling update will perform in different scenarios when updating the schduler. Use as a reference to observe the behavior and tune parameters accordingly.</p> <p>The scenarios assume that all rooms created in the surge will transition to ready in the next loop, which usually runs each 30s to 1min (depends on the configuration). In reality, depending on the number of rooms to surge, runtime might take longer to provision a node or game code might take a while to initialize and room actually becoming ready.</p> <p>Also, the number of occupied rooms will remain the same, which means that when an occupied Game Room from a previous version is deleted, a new one that was ready transitions to occupied just for the sake of simplicity in the computation of numbers of the scenario. In reality, the number of occupied rooms will vary throughout the cycle and rolling update will adjust to that as well.</p>"},{"location":"reference/RollingUpdate/#few-amount-of-game-room","title":"Few Amount of Game Room","text":"<ul> <li>readyTarget: 0.5</li> <li>maxSurge: 25%</li> </ul>"},{"location":"reference/RollingUpdate/#downscale","title":"Downscale","text":"loop ready occupied available desired desiredReady toSurge toBeDeleted 1 20 5 25 (0 new) 10 5 0 15 2 5 5 10 (0 new) 10 5 2 0 3 7 5 12 (2 new) 10 5 0 2 4 5 5 10 (2 new) 10 5 2 0 5 7 5 12 (4 new) 10 5 0 2 6 5 5 10 (4 new) 10 5 2 0 7 7 5 12 (6 new) 10 5 0 2 8 5 5 10 (6 new) 10 5 2 0 9 7 5 12 (8 new) 10 5 0 2 10 5 5 10 (8 new) 10 5 2 0 11 7 5 12 (10 new) 10 5 - 2 12 5 5 10 (10 new) 10 5 - -"},{"location":"reference/RollingUpdate/#upscale","title":"Upscale","text":"loop ready occupied available desired desiredReady toSurge toBeDeleted 1 5 20 25 (0 new) 40 20 10 0 2 15 20 35 (10 new) 40 20 10 0 3 25 20 45 (20 new) 40 20 5 5 4 25 20 45 (25 new) 40 20 5 5 5 25 20 45 (30 new) 40 20 5 5 6 35 20 45 (35 new) 40 20 5 5 7 25 20 45 (40 new) 40 20 0 5 8 20 20 40 (40 new) 40 20 0 0"},{"location":"reference/Scheduler/","title":"Scheduler","text":""},{"location":"reference/Scheduler/#what-is","title":"What is","text":"<p>Objectively, a Scheduler is a recipe, and contains all the information for creating  game rooms and forwarding rooms information to other services. </p> <p>Also, it's the core entity for operating game rooms in Maestro, since all game rooms are related to a specific scheduler.</p> <p>A game can have multiple schedulers, and each scheduler can have multiple game rooms up and running.</p> <pre><code>flowchart TD\n  etc1(\"...\")\n  etc2(\"...\")\n  etc3(\"...\")\n\n  subgraph game [Game]\n    subgraph scheduler_1 [Scheduler 1]\n      gameRoom1(\"Game Room (host:port)\")\n      gameRoom2(\"Game Room (host:port)\")\n      etc1\n    end\n    subgraph scheduler_2 [Scheduler 2]\n      gameRoom3(\"Game Room (host:port)\")\n      gameRoom4(\"Game Room (host:port)\")\n      etc2\n    end\n    etc3\n  end\n</code></pre>"},{"location":"reference/Scheduler/#how-to-operate","title":"How to Operate","text":"<p>To directly interact with a Scheduler, the user enqueues operations using the management API.</p> <p>These operations are responsible for creating a scheduler or newer versions, switching an active version, adding/removing rooms, etc.</p> <p>Because of that, everything that happens for a Scheduler can be tracked based on history of the operations executed for that scheduler and the order they were executed.</p>"},{"location":"reference/Scheduler/#versions","title":"Versions","text":"<p>A Scheduler have versions, and each time we want to change scheduler properties, we end-up creating a new version to it.</p> <p>Versions are directly calculated by Maestro, not sent by the client.</p> <p>The client can only switch the active version based on the versions created by Maestro. To switch to an specific version, see this.</p> <p>This version can either be a Minor or a Major change.</p> <ul> <li>Major version: Replace the game rooms in a switch active version event.</li> <li>Basically, any change under spec, that are related to the game room directly.</li> <li>Minor version: Don't replace game rooms in a switch active version event.</li> <li>Info such as MaxSurge or forwarders, that do not impact the game rooms.</li> </ul>"},{"location":"reference/Scheduler/#example","title":"Example","text":"<p>A complete Scheduler looks like this:</p> YAML <pre>\nname: scheduler-test\ngame: game-test\nstate: creating\nportRange:\n  start: 40000\n  end: 60000\nmaxSurge: 30%\nspec:\n  terminationGracePeriod: '100s'\n  containers:\n    - name: alpine\n      image: alpine\n      imagePullPolicy: IfNotPresent\n      command:\n        - /bin/sh\n        - '-c'\n        - &gt;-\n          apk add curl &amp;&amp; while true; do curl --request POST\n          {{maestro-rooms-api}}/scheduler/$MAESTRO_SCHEDULER_NAME/rooms/$MAESTRO_ROOM_ID/ping\n          --data-raw '{\"status\": \"ready\",\"timestamp\": \"12312312313\"}' &amp;&amp; sleep\n          1; done\n      environment:\n        - name: env-var-name\n          value: env-var-value\n        - name: env-var-field-ref\n          valueFrom:\n            fieldRef:\n              fieldPath: path\n        - name: secret-var-name\n          valueFrom:\n            secretKeyRef:\n              name: secret-name\n              key: secret-key\n      requests:\n        memory: 20Mi\n        cpu: 100m\n      limits:\n        memory: 200Mi\n        cpu: 200m\n      ports:\n        - name: port-name\n          protocol: tcp\n          port: 12345\n  toleration: maestro\n  affinity: maestro-dedicated\nforwarders:\n  - name: test\n    enable: true\n    type: gRPC\n    address: {{host}}\n    options:\n      timeout: '1000'\n      metadata: {}\nautoscaling:\n  enabled: true\n  min: 1\n  max: 10\n  policy:\n    type: roomOccupancy\n    parameters:\n      ...\n      // Will vary according to the policy type.\n        </pre> JSON <pre>\n{\n    \"name\": \"scheduler-test\",\n    \"game\": \"game-test\",\n    \"portRange\": {\n        \"start\": 40000,\n        \"end\": 60000\n    },\n    \"annotations\":{\n      \"imageregistry\":\"https://dockerhub.com/\"    \n    },\n    \"maxSurge\": \"30%\",\n    \"spec\": {\n        \"terminationGracePeriod\": '100s',\n        \"containers\": [\n            {\n                \"name\": \"alpine\",\n                \"image\": \"alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"command\": [\n                    \"/bin/sh\",\n                    \"-c\",\n                    \"apk add curl &amp;&amp; while true; do curl --request POST {{maestro-rooms-api}}/scheduler/$MAESTRO_SCHEDULER_NAME/rooms/$MAESTRO_ROOM_ID/ping --data-raw '{\"status\": \"ready\",\"timestamp\": \"12312312313\"}' &amp;&amp; sleep 1; done\"\n                ],\n                \"environment\": [\n                    {\n                        \"name\": \"env-var-name\",\n                        \"value\": env-var-value\n                    },\n                    {\n                        \"name\": \"env-var-field-ref\",\n                        \"valueFrom\": {\n                            \"fieldRef\": {\n                                \"fieldPath\": \"path\"\n                            }\n                        }\n                    },\n                    {\n                        \"name\": \"secret-var-name\",\n                        \"valueFrom\": {\n                            \"secretKeyRef\": {\n                                \"name\": \"secret-name\",\n                                \"key\": \"secret-key\"\n                            }\n                        }\n                    }\n                ],\n                \"requests\": {\n                    \"memory\": \"20Mi\",\n                    \"cpu\": \"100m\"\n                },\n                \"limits\": {\n                    \"memory\": \"200Mi\",\n                    \"cpu\": \"200m\"\n                },\n                \"ports\": [\n                    {\n                        \"name\": \"port-name\",\n                        \"protocol\": \"tcp\",\n                        \"port\": 12345,\n                    }\n                ]\n            }\n        ],\n        \"toleration\": \"maestro\",\n        \"affinity\": \"maestro-dedicated\"\n    },\n    \"forwarders\": [\n        {\n            \"name\": \"test\",\n            \"enable\": true,\n            \"type\": \"gRPC\",\n            \"address\": \"{{host}}\",\n            \"options\": {\n                \"timeout\": '1000',\n                \"metadata\": {}\n            }\n        }\n    ]\n    \"autoscaling\": {\n      \"enabled\": true,\n      \"min\": 10,\n      \"max\": 300,\n      \"policy\": {\n        \"type\": \"roomOccupancy\",\n        \"parameters\": {\n          ...\n          // Will vary according to the policy type.\n        }\n      }\n    }\n}\n    </pre>"},{"location":"reference/Scheduler/#structure","title":"Structure","text":"<p>The scheduler is represented as:</p> <pre><code>name: String\ngame: String\ncreatedAt: Timestamp\nmaxSurge: String | Integer\nportRange: PortRange\nforwarders: Forwarders\nautoscaling: Autoscaling\nspec: Spec\nannotation: Map\n</code></pre> <ul> <li>Name: Scheduler name. This name is unique and will be the same name used for the kubernetes namespace. It's   offered by the user in the creation and cannot be changed in the future. It's used as an ID for the scheduler;</li> <li>game: Name of the game which will use the scheduler. The game is important since it's common to use multiple   schedulers for a specific game. So you probably will want to fetch all the schedulers from a game;</li> <li>createdAt: Info about the scheduler creation time. Cannot be altered by the user;</li> <li>maxSurge: Value represented in percentage (%) or Integer. Offered by the user on the scheduler creation. Can be   altered anytime. Used by Maestro to replace pods. Ex: If maxSurge = 3,   the Switch Active Version (Major change) operation will replace pods from 3 to   3;</li> <li>portRange: Range of ports that can be used by Maestro to create GRUs for the specified scheduler. Can be altered   by the user anytime. More info here; Mutually exclusive with the Spec.Containers.Ports.HostPortRange configuration.</li> <li>forwarders: Maestro can pass ahead info sent by the game rooms, such as Ping (Ready, Occupied, Terminating...),   player and rooms events. The receivers can be configured here. More info here;</li> <li>autoscaling: Optional autoscaling policy configuration. More info here;</li> <li>spec: Specifications about the game rooms managed by the scheduler, such as containers and environment variables   used by them, limits and images. More info here.</li> <li>annotations: Allows annotations for the scheduler's game room. Know more about annotations on   Kubernetes here</li> </ul>"},{"location":"reference/Scheduler/#portrange","title":"PortRange","text":"<p>The PortRange is used to select a random port for a GRU between start and end.</p> <ul> <li>PortRange cannot be null or empty</li> </ul> <p>Check if the ports offered are available and can be used.  A firewall rule, for example, can affect the connection to the Game Room in the specific port.</p> <p>It is represented as:</p> <pre><code>start: Integer\nend: Integer\n</code></pre> <p>You must configure the port range either in <code>.PortRange</code> or in <code>.Spec.Containers.Ports.HostPortRange</code> - they are mutually exclusive configurations. If you want to configure a single port range for all container ports, use <code>.PortRange</code>, otherwise use <code>HostPortRange</code> in every Spec to configure the port range for each one - this is useful when avoiding port conflicts on different protocols in case the network doesn't support it, for example.</p>"},{"location":"reference/Scheduler/#forwarders","title":"Forwarders","text":"<p>Forwarders are configured to pass ahead info offered by the game rooms to Maestro.  More than one forwarder can be configured for a scheduler.</p> <ul> <li>Can be an empty list.</li> </ul> <p>It is represented as:</p> <pre><code>- name: String\n  enable: Bool\n  type: String\n  address: String\n  options:\n    timeout: Integer\n    metadata: Object\n</code></pre> <ul> <li>name: Name of the forwarder. Used only for reference (visibility and recognition);</li> <li>enable: Toggle to easily enable/disable the forwarder;</li> <li>type: Type of the forwarder. Right now, only accepts gRPC;</li> <li>address: Address used by the scheduler to forward events. E.g. 'api.example.com:8080';</li> <li>options: Optional parameters.<ul> <li>timeout: Timeout value for an event to successfully be forwarded;</li> <li>metadata: Object that can contain any useful information for the game team. Will be forwarded with the events from Maestro.</li> </ul> </li> </ul>"},{"location":"reference/Scheduler/#spec","title":"Spec","text":"<p>Contains vital information about the game rooms. Be aware that the spec is the most related aspect of the scheduler interacting with the runtime.  It might be important to understand the basics of kubernetes before deep diving into the Maestro scheduler itself.</p> <ul> <li>Cannot be empty or null.</li> </ul> <p>It is represented as:</p> <pre><code>terminationGracePeriod: String\ncontainers: Containers\ntoleration: String\naffinity: String\n</code></pre> <ul> <li>terminationGracePeriod: Required string value. Must be greater than 0 and have the unit set, i.e \"100s\". When a game room receives the signal to be deleted, it will take this value (in seconds) to be completely deleted;</li> <li>containers: Contain the information about the game room, such as the image and environment variables. This is a list since the game room can be compounded by more than two containers;</li> <li>toleration: Kubernetes specific. Represents the toleration value for all GRUs on the scheduler. See more;</li> <li>affinity: Kubernetes specific. Represents the affinity value for all GRUs on the scheduler. See more.</li> </ul>"},{"location":"reference/Scheduler/#containers","title":"Containers","text":"<p>Contain the information about the game room, such as the image and environment variables.</p> <ul> <li>Cannot be an empty list.</li> </ul> <p>It is represented as:</p> <pre><code>- name: String\n  image: String\n  imagePullPolicy: String\n  command: Array&lt;String&gt;\n  environment: EnvironemntVariables\n  requests:\n    memory: String\n    cpu: String\n  limits:\n    memory: String\n    cpu: String\n  ports: Ports\n</code></pre> <ul> <li>name: Name of the container, used only for reference and can be changed by the user anytime.</li> <li>image: Docker image to be used for the container. Represented as a link.</li> <li>imagePullPolicy: Kubernetes specific. See here for reference.</li> <li>command: List of commands that should be executed by the image on execution. E.g. here.</li> <li>environment: List of environment variables. See here.</li> <li>requests and limits: Kubernetes specific. See here.</li> <li>ports: The list of ports your game server exposes. See here.</li> </ul>"},{"location":"reference/Scheduler/#environment-variables","title":"Environment Variables","text":"<p>List of environment variables used by the GRU.</p> <ul> <li>Can be an empty list.</li> </ul> <p>There are, now, 3 supported formats by Maestro:</p> <ul> <li>Simple name-value format.</li> </ul> With Name/Value <pre>\n- name: String\n  value: String</pre> <ul> <li>Exposing pod fields (kubernetes specific). See here.</li> </ul> With FieldRef/FieldPath <pre>\n- name: String\n  valueFrom:\n    fieldRef:\n      fieldPath: String</pre> <ul> <li>Secrets as environment variables (kubernetes specific). See here.</li> </ul> With Secret <pre>\n- name: String\n  valueFrom:\n    secretKeyRef:\n      name: String\n      key: String</pre>"},{"location":"reference/Scheduler/#ports","title":"Ports","text":"<p>The list of ports your game server exposes.</p> <ul> <li>Can be an empty list.</li> </ul> <p>It is represented as:</p> <pre><code>- name: String\n  protocol: String\n  port: Integer\n  hostPortRange: PortRange\n</code></pre> <ul> <li>name: Name of the port. Facilitates on recognition;</li> <li>protocol: Port protocol. Can be UDP, TCP or SCTP.;</li> <li>port: The port exposed.</li> <li>hostPortRange: The port range for the port to be allocated in the host. Mutually exclusive with the port range configured in the root structure.</li> </ul>"},{"location":"tutorials/Autoscaling/","title":"Autoscaling","text":""},{"location":"tutorials/Autoscaling/#configuring-scheduler-autoscaling","title":"Configuring Scheduler Autoscaling","text":""},{"location":"tutorials/Autoscaling/#prerequisites","title":"Prerequisites","text":"<ul> <li>Have a game room container image that communicates with maestro through Maestro's rooms API</li> </ul>"},{"location":"tutorials/Autoscaling/#learning-outcomes","title":"Learning Outcomes","text":"<p>After finishing this tutorial you will understand how:</p> <ul> <li>to configure autoscaling policies for your scheduler</li> </ul>"},{"location":"tutorials/Autoscaling/#what-is","title":"What is","text":"<p>Autoscaling is an optional feature in which the user can choose and parametrize different autoscaling policies that maestro will use to automatically scale the number of rooms in the scheduler.</p> <p>Maestro has an internal process that periodically keeps checking if it needs to create or delete game rooms for the given scheduler, if autoscaling is not configured or enabled, it will always try to maintain the current number of rooms equal to roomsReplicas scheduler property. If autoscaling is configured and enabled, it will use the configured autoscaling policy to decide if it needs to scale up (create more rooms),  scale down (delete rooms) or do nothing.</p> <pre><code>  flowchart TD\n    finish((End))\n    add_rooms_operation(Enqueue add rooms operation)\n    remove_rooms_operation(Enqueue remove rooms)\n    use_rooms_replicas(Use rooms replicas to calculate the desired number of rooms)\n    autoscaling_enabled{Autoscaling configured and enabled?}\n    decide_operation{Compare current number of rooms with the desired amount.}\n    use_autoscaling[Use Autoscaling policy to calculate the desired number of rooms coerced in min-max range]\n    autoscaling_enabled -- No --&gt; use_rooms_replicas;\n    autoscaling_enabled -- Yes --&gt; use_autoscaling;\n    use_autoscaling --&gt; decide_operation;\n    use_rooms_replicas --&gt; decide_operation;\n    decide_operation --  desired &gt; actual --&gt; add_rooms_operation --&gt; finish;\n    decide_operation --  desired == actual --&gt; finish;\n    decide_operation --  desired &lt; actual --&gt; remove_rooms_operation --&gt; finish;\n</code></pre> <p>Currently, the sync interval is configured by environment variable <code>MAESTRO_WORKERS_HEALTHCONTROLLERINTERVAL</code>.</p> <p>By default, the scheduler does not have autoscaling configured.</p>"},{"location":"tutorials/Autoscaling/#how-to-configure-and-enable-autoscaling","title":"How to configure and enable autoscaling","text":"<p>To get autoscaling working in your scheduler, firstly you need to configure an autoscaling policy and enable it, this autoscaling configuration resides in the root of the scheduler structure itself.</p> YAML version <pre>\nname: String\ngame: String\n...\nautoscaling:\n  enabled: true\n  min: 1\n  max: 10\n  cooldown: 60\n  policy:\n    type: roomOccupancy\n    parameters:\n      ...\n      // Will vary according to the policy type.\n        </pre> JSON version <pre>\n{\n  \"name\": \"test\",\n  \"game\": \"multiplayer\",\n  ...\n  \"autoscaling\": {\n    \"enabled\": true,\n    \"min\": 10,\n    \"max\": 300,\n    \"cooldown\": 60,\n    \"policy\": {\n      \"type\": \"roomOccupancy\",\n      \"parameters\": {\n        ...\n        // Will vary according to the policy type.\n      }\n    }\n  }\n}\n        </pre> <ul> <li>enabled [boolean]: A value that can be true or false, indicating if the autoscaling feature is enabled/disabled for the given scheduler. Default: false.</li> <li>min [integer]: Minimum number of rooms the scheduler should have, it must be greater than zero. For zero value, disable autoscaling and set \"roomsReplicas\" to 0.</li> <li>max [integer]: Maximum number of rooms the scheduler can have. It must be greater than min, or can be -1 (to have no limit).</li> <li>cooldown [integer]: The cooldown period (in seconds) between downscaling operations. This is useful to avoid scale down too fast,    which can cause a lot of rooms to be deleted in a short period of time.</li> <li>policy [struct] : This field holds information regarding the autoscaling policy that will be used if the autoscaling feature is enabled:</li> <li>type [string]:  Define the policy type that will be used, must be one of the policy types maestro provides.</li> <li>parameters [struct]: This field will contain arbitrary fields that will vary according to the chosen policy type.</li> </ul>"},{"location":"tutorials/Autoscaling/#policy-types","title":"Policy Types","text":"<p>Maestro has a set of predefined policy types that can be used to configure the autoscaling, each policy will implement a specific strategy for calculating the desired number of rooms and will have its configurable parameters.</p> <p>Note: Policy types are mutually exclusive. The parameters used will depend on the value of <code>autoscaling.policy.type</code>.</p>"},{"location":"tutorials/Autoscaling/#room-occupancy-policy","title":"Room Occupancy Policy","text":"<p>The basic concept of this policy is to scale the scheduler up or down based on the actual room occupancy rate, by defining a \"buffer\" percentage of ready rooms that Maestro must keep. The desired number of rooms will be given by the following formula:</p> <p><code>desiredNumberOfRooms = \u2308(numberOfOccupiedRooms/ (1- readyTarget) )\u2309</code></p> <p>So basically Maestro will constantly try to maintain a certain percentage of rooms in ready state, by looking at the actual room occupancy rate (number of rooms in occupied state).</p>"},{"location":"tutorials/Autoscaling/#room-occupancy-policy-parameters","title":"Room Occupancy Policy Parameters","text":"<ul> <li>readyTarget [float]: The percentage (in decimal value) of rooms that Maestro should try to keep in ready state, must be a value greater than 0 and less than 1.</li> <li>downThreshold [float]: It adjusts how often maestro scale down Game Rooms, where 0.99 means that maestro will always scale down a Game Room when it is free (respecting the readyTarget), and a value close to 0 means that maestro will almost never scale down. Must be a value greater than 0 and less than 1.</li> </ul>"},{"location":"tutorials/Autoscaling/#example","title":"Example","text":"YAML version <pre>\nname: String\ngame: String\n...\nautoscaling:\n  enabled: true\n  min: 1\n  max: 10\n  policy:\n    type: roomOccupancy\n    parameters:\n      roomOccupancy:\n        readyTarget: 0.5\n        </pre> JSON version <pre>\n{\n  \"autoscaling\": {\n    \"enabled\": true,\n    \"min\": 10,\n    \"max\": 300,\n    \"policy\": {\n      \"type\": \"roomOccupancy\",\n      \"parameters\": {\n        \"roomOccupancy\": {\n          \"readyTarget\": 0.5\n        }\n      }\n    }\n  }\n}\n        </pre> <p>Below are some simulated examples of how the room occupancy policy will behave:</p> <p>Note that the autoscaling decision will always be limited by the min-max values! .</p> totalRooms occupiedRooms readyTarget desiredNumberOfRooms autoscalingDecision 100 80 0.5 160 Scale Up: +60 100 50 0.5 100 Do Nothing: 0 100 30 0.5 60 Scale Down: -40 50 40 0.3 58 Scale Up: +8 50 35 0.3 50 Do Nothing: 0 50 10 0.3 15 Scale Down: -35 10 5 0.9 50 Scale Up: +40 10 1 0.9 10 Do Nothing: 0 10 1 0.8 5 Scale Down: -5 5 5 0.1 6 Scale Up: +1 1 1 0.3 2 Scale Up: +1 2 2 0.9 20 Scale Up: +18"},{"location":"tutorials/Autoscaling/#fixed-buffer-amount-policy","title":"Fixed Buffer Amount Policy","text":"<p>The Fixed Buffer Amount policy maintains a fixed number of rooms on top of the currently occupied rooms. This policy is useful when you want to ensure a consistent buffer of available rooms regardless of the occupancy rate.</p> <p>The desired number of rooms will be given by the following formula:</p> <pre><code>desiredNumberOfRooms = numberOfOccupiedRooms + fixedBufferAmount\n</code></pre> <p>Respecting autoscaling boundaries (min and max):</p> <pre><code>if desiredNumberOfRooms &lt; autoscaling.Min {\n  desiredNumberOfRooms = autoscaling.Min\n}\n\nif autoscaling.Max != -1 &amp;&amp; desiredNumberOfRooms &gt; autoscaling.Max {\n  desiredNumberOfRooms = autoscaling.Max\n}\n</code></pre> <p>Maestro will constantly try to maintain the specified fixed amount of rooms in addition to the occupied rooms, ensuring there are always enough rooms available for new players.</p>"},{"location":"tutorials/Autoscaling/#fixed-buffer-policy-parameters","title":"Fixed Buffer Policy Parameters","text":"<ul> <li>fixedBufferAmount [integer]: The fixed number of rooms that Maestro should maintain on top of the occupied rooms. Must be a value greater than 0 and less than the maximum number of rooms (max) when max is greater than 0.</li> </ul>"},{"location":"tutorials/Autoscaling/#example_1","title":"Example","text":"YAML version <pre>\nname: String\ngame: String\n...\nautoscaling:\n  enabled: true\n  min: 1\n  max: 100\n  policy:\n    type: fixedBuffer\n    parameters:\n      fixedBuffer:\n        amount: 50\n        </pre> JSON version <pre>\n{\n  \"autoscaling\": {\n    \"enabled\": true,\n    \"min\": 1,\n    \"max\": 100,\n    \"policy\": {\n      \"type\": \"fixedBuffer\",\n      \"parameters\": {\n        \"fixedBuffer\": {\n          \"amount\": 50\n        }\n      }\n    }\n  }\n}\n        </pre> Coexisting with RoomOccupancy <pre>\n{\n  \"autoscaling\": {\n    \"enabled\": true,\n    \"min\": 1,\n    \"max\": 100,\n    \"policy\": {\n      \"type\": \"fixedBuffer\",\n      \"parameters\": {\n        \"fixedBuffer\": {\n          \"amount\": 50\n        },\n        \"roomOccupancy\": {\n          \"readyTarget\": 0.2,\n          \"downThreshold\": 0.9\n        }\n      }\n    }\n  }\n}\n        </pre> <p>Below are some simulated examples of how the fixed buffer amount policy will behave:</p> <p>Note that the autoscaling decision will always be limited by the min-max values!</p> totalRooms occupiedRooms fixedBufferAmount desiredNumberOfRooms autoscalingDecision 100 80 50 130 Scale Up: +30 100 50 50 100 Do Nothing: 0 100 30 50 80 Scale Down: -20 50 40 20 60 Scale Up: +10 50 30 20 50 Do Nothing: 0 50 10 20 30 Scale Down: -20 10 5 10 15 Scale Up: +5 10 0 10 10 Do Nothing: 0 10 0 5 5 Scale Down: -5 5 5 5 10 Scale Up: +5 1 1 3 4 Scale Up: +3 2 2 8 10 Scale Up: +8"},{"location":"tutorials/BestPractices/","title":"Maestro Schedulers' best practices","text":""},{"location":"tutorials/BestPractices/#scheduler-rollouts","title":"Scheduler rollouts","text":"<p>This document defines best practices for games on how to operate Maestro during scheduler\u2019s transitions to avoid occurrences of no rooms available. As a rule of thumb, the lower the maxSurge, the safer the rollout. Maestro creates and removes fewer % rooms (compared to the total) in each iteration (1 min interval by default). Thus, a maxSurge of 10% will take ~10-20 minutes to complete the rollout. If time is not a constraint, aim to use a lower maxSurge to ensure the smoothest process during major rollouts (any change within spec).</p>"},{"location":"tutorials/BestPractices/#1-when-changing-schedulers","title":"1. When Changing Schedulers:","text":""},{"location":"tutorials/BestPractices/#gradual-rollout","title":"Gradual Rollout:","text":"<p>Scenario: Players are gradually joining a new scheduler, possibly due to an Android or iOS canary release.</p> <p>Recommendation: Increase the ready target to at least 40%. This ensures sufficient capacity during the transition, reducing the risk of room shortages as the new scheduler ramps up.</p>"},{"location":"tutorials/BestPractices/#forced-rollout","title":"Forced Rollout:","text":"<p>Scenario: Players will be manually disconnected from the current scheduler, and all new matches will be routed to the new scheduler.</p> <p>Recommendation: Set the minimum number of rooms to match or exceed the baseline of the previous scheduler. This provides a buffer to maintain stability during the transition.</p>"},{"location":"tutorials/BestPractices/#2-post-rollout-adjustments","title":"2. Post-Rollout Adjustments:","text":""},{"location":"tutorials/BestPractices/#rollback-ready-target-min-room-count","title":"Rollback Ready Target / Min Room Count:","text":"<p>Once the rollout has finished, you can rollback the ready target and/or minimum number of rooms to their standard levels.</p>"},{"location":"tutorials/BestPractices/#recommended-defaults-for-games","title":"Recommended defaults for games","text":"<p>This section outlines a few recommended defaults games that uses Maestro.These are only recommendations, and they can definitely change if your game's requirements change. Use them as a guideline to define what works best for you.</p>"},{"location":"tutorials/BestPractices/#minimum-number-of-rooms","title":"Minimum number of rooms","text":"<p>When defining the minimum number of rooms for a scheduler, one good rule of thumb, is checking the rooms variation over the course of a week, and setting it as the local minimum of this eval window. Ex.: let's say, that during the span of a week, the scheduler had the following amount of rooms:</p> Day Rooms day 1 100 rooms day 2 50 rooms day 3 25 rooms day 4 50 rooms day 5 100 rooms day 6 100 rooms day 7 50 rooms <p>Min rooms should be 25, since it was the smallest value during the evaluation window.</p> <p>They were defined by getting the minimum number of rooms that each region had during a 1 week avaliation window.</p>"},{"location":"tutorials/Development/","title":"Developers","text":""},{"location":"tutorials/Development/#development","title":"Development","text":""},{"location":"tutorials/Development/#setting-up-the-environment","title":"Setting up the environment","text":""},{"location":"tutorials/Development/#grpc-gateway","title":"Grpc gateway","text":"<p>In order to run make generate with success, you need to have grpc-gateway dependencies installed with the following command:</p> <pre><code>go install \\\n    github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-grpc-gateway \\\n    github.com/grpc-ecosystem/grpc-gateway/v2/protoc-gen-openapiv2 \\\n    google.golang.org/protobuf/cmd/protoc-gen-go \\\n    google.golang.org/grpc/cmd/protoc-gen-go-grpc\n</code></pre>"},{"location":"tutorials/Development/#golang-version","title":"Golang version","text":"<p>The project requires golang version 1.20 or higher.</p>"},{"location":"tutorials/Development/#building-and-running","title":"Building and running","text":"<ol> <li>Run <code>make setup</code> to get all required modules</li> <li>Run <code>make generate</code> to generate mocks, protos and wire (dependency injection)</li> <li>Run <code>make deps/up</code> to startup service dependencies</li> <li>Run <code>make migrate</code> to migrate database with the most updated schema</li> </ol>"},{"location":"tutorials/Development/#running-tests","title":"Running tests","text":"<ol> <li>Run <code>make run/unit-tests</code> to run all unit tests</li> <li>Run <code>make run/integration-tests</code> to run all integration tests</li> <li>Run <code>make run/e2e-tests</code> to run all E2E tests. NOTE: Currently it is not    possible to run it with the development environment set. This command will    stop the dev dependencies before running.</li> <li>Run <code>make lint</code> to run all registered linters</li> </ol>"},{"location":"tutorials/Development/#running-locally","title":"Running locally","text":"<p>To help you get along with Maestro, by the end of this section you should have a scheduler up and running.</p>"},{"location":"tutorials/Development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Golang v1.20+</li> <li>Linux/MacOS environment</li> <li>Docker</li> </ul>"},{"location":"tutorials/Development/#clone-repository","title":"Clone Repository","text":"<p>Clone the repository to your favorite folder.</p>"},{"location":"tutorials/Development/#getting-maestro-up-and-running","title":"Getting Maestro up and running","text":"<p>For this step, you need docker running on your machine.</p> <p>WARNING: Ensure using cgroupv1</p> <p>K3s needs to use the deprecated <code>cgroupv1</code>, to successfully run the project in your machine ensure that your current docker use this version.</p> <p>In the folder where the project was cloned, simply run:</p> <pre><code>make maestro/start\n</code></pre> <p>This will build and start all containers needed by Maestro, such as databases and maestro-modules. This will also start all maestro components, including rooms api, management api, runtime watcher, and execution worker.</p> <p>Because of that, be aware that it might take some time to finish.</p>"},{"location":"tutorials/Development/#find-rooms-api-address","title":"Find rooms-api address","text":"<p>To simulate a game room, it's important to find the address of running rooms-api on the local network.</p> <p>To do that, with Maestro containers running, simply use:</p> <pre><code>docker inspect -f '{{range.NetworkSettings.Networks}}{{.Gateway}}{{end}}' {{ROOMS_API_CONTAINER_NAME}}\n</code></pre> <p>This command should give you an IP address. This IP is important because the game rooms will use it to communicate their status.</p>"},{"location":"tutorials/Development/#create-a-scheduler","title":"Create a scheduler","text":"<p>If everything is working as expected now, each Maestro-module is up and running. Use the command below to create a new scheduler:</p> <p>Be aware to change the {{ROOMS_API_ADDRESS}} for the one found above.</p> <pre><code>curl --request POST \\\n  --url http://localhost:8080/schedulers \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"name\": \"scheduler-run-local\",\n    \"game\": \"game-test\",\n    \"state\": \"creating\",\n    \"portRange\": {\n        \"start\": 1,\n        \"end\": 1000\n    },\n    \"maxSurge\": \"10%\",\n    \"spec\": {\n        \"terminationGracePeriod\": \"100s\",\n        \"containers\": [\n            {\n                \"name\": \"alpine\",\n                \"image\": \"alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"command\": [\n                    \"sh\",\n                    \"-c\",\n                    \"apk add curl &amp;&amp; while true; do curl --request PUT {{ROOMS_API_ADDRESS}}:8070/scheduler/$MAESTRO_SCHEDULER_NAME/rooms/$MAESTRO_ROOM_ID/ping --data-raw '\\''{\\\"status\\\": \\\"ready\\\",\\\"timestamp\\\": \\\"12312312313\\\"}'\\'' &amp;&amp; sleep 5; done\"\n                ],\n                \"environment\": [],\n                \"requests\": {\n                    \"memory\": \"100Mi\",\n                    \"cpu\": \"100m\"\n                },\n                \"limits\": {\n                    \"memory\": \"200Mi\",\n                    \"cpu\": \"200m\"\n                },\n                \"ports\": [\n                    {\n                        \"name\": \"port-name\",\n                        \"protocol\": \"tcp\",\n                        \"port\": 12345\n                    }\n                ]\n            }\n        ],\n        \"toleration\": \"\",\n        \"affinity\": \"\"\n    },\n    \"forwarders\": []\n}'\n</code></pre>"},{"location":"tutorials/Development/#congratulations","title":"Congratulations","text":"<p>If you followed the steps above you have Maestro running in your local machine, and with a scheduler to try different operations on it. Feel free to explore the available endpoints in the API hitting directly the management-API.</p> <p>If you have any doubts or feedbacks regarding this process, feel free to reach out in Maestro's GitHub repository and open an issue/question.</p>"},{"location":"tutorials/Development/#local-environment-setup","title":"Local Environment Setup","text":"<p>To run Maestro locally for development or testing, you'll need the following tools installed:</p> <ul> <li>Go (version 1.23 or higher)</li> <li>Docker</li> <li>Docker Compose</li> <li>kubectl (Kubernetes CLI) - Optional, for interacting with the k3d cluster directly.</li> <li>Make</li> </ul> <p>(Note: <code>k3d</code> itself is now managed via Docker Compose as part of the local dependencies, so direct installation of <code>k3d</code> CLI is no longer a prerequisite for basic <code>make</code> target usage.)</p>"},{"location":"tutorials/Development/#running-maestro-locally-with-k3d-managed-by-docker-compose","title":"Running Maestro Locally with k3d (Managed by Docker Compose)","text":"<ol> <li> <p>Set up dependencies (Postgres, Redis &amp; k3d Kubernetes cluster):     The <code>docker-compose.yaml</code> file at the root of the project defines services for Postgres, Redis, and <code>k3d</code>. The <code>k3d</code> service will automatically create a Kubernetes cluster named <code>maestro-dev</code> and export its kubeconfig to <code>e2e/framework/maestro/.k3d-kubeconfig.yaml</code>.</p> <p>To start these dependencies, including the k3d cluster: <code>bash make deps/up</code> Alternatively, to only start/ensure the k3d cluster is up: <code>bash make k3d/up</code> This command will also wait until the kubeconfig file is ready. The <code>maestro-dev</code> cluster will have port <code>38080</code> on your host mapped to the cluster's ingress (port 80 on the load balancer).</p> </li> <li> <p>Start Maestro Application Services:     Once the k3d cluster is running and its kubeconfig is available (handled by <code>make k3d/up</code>), you can start the Maestro application services (management-api, rooms-api, etc.). These are defined in <code>e2e/framework/maestro/docker-compose.yml</code>.</p> <p>The recommended way to start everything, including dependencies and Maestro services, is: <code>bash make maestro/start</code> This target handles the correct order of operations: starts k3d, ensures kubeconfig is present, builds Maestro, runs migrations, and then starts the Maestro services using the <code>e2e/framework/maestro/docker-compose.yml</code> file. The Maestro services in this compose file are configured to use the kubeconfig generated by the <code>k3d</code> service.</p> <p>If you need to start them manually after <code>make k3d/up</code> and <code>make build</code>: ```bash</p> </li> <li> <p>Build and Run Maestro components (alternative to full Docker Compose setup for services):     If you prefer to run Maestro components (e.g., <code>management-api</code>, <code>worker</code>) directly on your host for development (after <code>make deps/up</code> has started Postgres, Redis, and k3d):     <code>bash     make build</code>     Then run individual components, e.g.:     <code>bash     # Ensure KUBECONFIG points to the k3d cluster     export KUBECONFIG=\"$(pwd)/e2e/framework/maestro/.k3d-kubeconfig.yaml\"     MAESTRO_INTERNALAPI_PORT=8081 MAESTRO_API_PORT=8080 go run main.go start management-api -l development</code>     These components will connect to Postgres and Redis (from the root <code>docker-compose</code>) and the k3d Kubernetes cluster (using the exported KUBECONFIG).</p> </li> <li> <p>Running Tests:</p> <ul> <li>Unit Tests: <code>make run/unit-tests</code></li> <li>Integration Tests: <code>make run/integration-tests</code> (These will run against the k3d cluster managed by Docker Compose)</li> <li>E2E Tests: <code>make run/e2e-tests</code></li> </ul> </li> <li> <p>Stopping the local k3d cluster and dependencies:     To delete the <code>maestro-dev</code> k3d cluster and remove its Docker Compose service:     <code>bash     make k3d/down</code>     To stop all dependencies defined in the root <code>docker-compose.yaml</code> (Postgres, Redis, k3d):     <code>bash     make deps/down</code>     To stop the Maestro application services defined in <code>e2e/framework/maestro/docker-compose.yml</code>:     <code>bash     docker-compose -f e2e/framework/maestro/docker-compose.yml down</code>     The <code>make maestro/down</code> target handles stopping both the k3d cluster and the Maestro application services.</p> </li> </ol>"},{"location":"tutorials/Development/#ensure-migrations-are-run-if-its-the-first-time-or-after-schema-changes","title":"Ensure migrations are run if it's the first time or after schema changes","text":""},{"location":"tutorials/Development/#maestro_migration_pathfileinternalservicemigrations-go-run-maingo-migrate","title":"MAESTRO_MIGRATION_PATH=\"file://internal/service/migrations\" go run main.go migrate","text":""},{"location":"tutorials/Development/#the-make-maestrostart-target-handles-migrations","title":"(The make maestro/start target handles migrations)","text":"<p>docker-compose -f e2e/framework/maestro/docker-compose.yml up -d management-api rooms-api worker runtime-watcher # Add other services as needed ```</p>"},{"location":"tutorials/Development/#ci-environment","title":"CI Environment","text":"<p>Note: In our CI environment (GitHub Actions), we use KinD (Kubernetes in Docker) to run integration and runtime-integration tests. This provides a fresh Kubernetes cluster for each test run.</p>"},{"location":"tutorials/Development/#project-overview","title":"Project overview","text":""},{"location":"tutorials/EventsForwarding/","title":"Events Forwarding","text":""},{"location":"tutorials/EventsForwarding/#configuring-events-forwarding","title":"Configuring Events Forwarding","text":""},{"location":"tutorials/EventsForwarding/#prerequisites","title":"Prerequisites","text":"<ul> <li>Have a game room container image that communicates with maestro through Maestro's rooms API</li> </ul>"},{"location":"tutorials/EventsForwarding/#learning-outcomes","title":"Learning Outcomes","text":"<p>After finishing this tutorial you will understand how:</p> <ul> <li>to configure your room (ping) and player events to be forwarded to an external service (e.g. a matchmaking service)</li> </ul>"},{"location":"tutorials/EventsForwarding/#what-is","title":"What is","text":"<p>Events forwarding is an optional feature in which every room event or player event is forwarded to an external service.</p> <p>Through rooms API, Maestro provides several endpoints for receiving events from the game rooms. These events can be either room events (like room changing state from ready to occupied) or player events (like player joining or leaving the room). Maestro rely only on room events for managing the game rooms, player events endpoint is designed to be used exclusively with the events forwarding feature, since maestro does not depend on this information.</p> <p>Usually Maestro is used with a Matchmaking service, and a matchmaking service generally will need to keep up-to-date with the pool of game rooms that are available or not. Events forwarding feature exists for facilitating this integration, even being possible to make game rooms communicate with matchmaker directly.</p>"},{"location":"tutorials/EventsForwarding/#how-to-configure-and-enable-events-forwarder","title":"How to configure and enable events forwarder","text":"<p>To get events forwarding working in your scheduler, firstly you need to configure the events forwarder and enable it, this forwarder configuration resides in the root of the scheduler structure itself.</p> YAML version <pre>\nname: String\ngame: String\n...\nforwarders:\n  - name: matchmaking\n    enable: true\n    type: gRPC\n    address: 'external-matchmaker.svc.cluster.local:80'\n    options:\n      timeout: '1000'\n      metadata:\n        ...\n        // Will vary according to the policy type.\n        </pre> JSON version <pre>\n{\n  \"name\": \"String\",\n  \"game\": \"String\",\n  ...\n  \"forwarders\": [\n    {\n      \"name\": \"matchmaking\",\n      \"enable\": true,\n      \"type\": \"gRPC\",\n      \"address\": \"external-matchmaker.svc.cluster.local:80\",\n      \"options\": {\n        \"timeout\": \"1000\",\n        \"metadata\": {\n            ...\n            // Will vary according to the user needs.\n        } \n      }\n    }\n  ]\n}\n        </pre> <ul> <li>name: Name of the forwarder. Used only for reference (visibility and recognition);</li> <li>enable: Toggle to easily enable/disable the forwarder;</li> <li>type: Type of the forwarder. Right now, only accepts gRPC;</li> <li>address: Address used by the scheduler to forward events. E.g. 'api.example.com:8080';</li> <li>options: Optional parameters.</li> <li>timeout: Timeout value for an event to successfully be forwarded;</li> <li>metadata: Arbitrary metadata object that can contain any data that will be embedded in all event that is forwarded.</li> </ul>"},{"location":"tutorials/EventsForwarding/#events-forwarding-types","title":"Events Forwarding Types","text":"<p>Currently, Maestro only supports gRPC forwarder type.</p>"},{"location":"tutorials/EventsForwarding/#grpc","title":"GRPC","text":"<p>This event forwarding type uses the GRPCForwarder service proto definition to forward events, this means that the external service should use gRPC protocol and implement this service to receive events.</p>"},{"location":"tutorials/EventsForwarding/#response","title":"Response","text":"<p>Maestro expects the forwarder event response to return a HTTP code, which is mapped internally to a gRPC code. This mapping is done in the handlerGrpcClientResponse function</p>"},{"location":"tutorials/EventsForwarding/#client-configuration","title":"Client Configuration","text":"<p>The GRPC forwarder uses a client that will dial into the address of the forwarder configured in the scheduler and forward events to it. However, we need to configure a KeepAlive mechanism so we constantly send HTTP/2 ping frames on the channel and detect broken connections. Without a KeepAlive mechanism, broken TCP connections are only refreshed when Kernel kills the fd responsible due to inactivity, which can take up to 20 minutes.</p> <p>Thus, there are some configurations that you can do to tweak the KeepAlive configuration. Those configs can be set either as an env var or in the <code>config.yaml</code>:</p> <ul> <li><code>adapters.grpc.keepAlive.time</code>: After a duration of this time if the client doesn't see any activity it pings the server to see if the transport is still alive. If set below 10s, a minimum value of 10s will be used instead. Defaults to 30s.</li> <li><code>adapters.grpc.keepAlive.timeout</code>: After having pinged for keepalive check, the client waits for a duration of Timeout and if no activity is seen even after that the connection is closed. Defaults to 5s.</li> </ul> <p>GRPC Official Doc Reference</p> <p>GRPC Internals Reference</p>"},{"location":"tutorials/GettingStarted/","title":"Getting Started","text":""},{"location":"tutorials/GettingStarted/#getting-started-guide","title":"Getting Started Guide","text":""},{"location":"tutorials/GettingStarted/#prerequisites","title":"Prerequisites","text":"<ul> <li>Have a game room container image</li> </ul>"},{"location":"tutorials/GettingStarted/#learning-outcomes","title":"Learning Outcomes","text":"<p>After finishing this tutorial you will understand how:</p> <ul> <li>to set up your game room to communicate its health status with maestro</li> <li>to configure a new scheduler in maestro with a fixed number of replicas</li> </ul>"},{"location":"tutorials/GettingStarted/#configuring-your-game-room","title":"Configuring your game room","text":"<p>For Maestro to be able to manage your game rooms, you need to ensure that your game room sends a periodic heartbeat to maestro. This heartbeat is what we call <code>ping</code>, in which the room is able to inform its status (such as <code>ready</code> or <code>occupied</code>) to maestro.</p> <p>For this, you can use maestro-client sdk, if you are using unity, or you can call Maestro rooms API directly using two env vars that are configured in every game room managed by maestro by default.</p> <p><code>PUT scheduler/$MAESTRO_SCHEDULER_NAME/rooms/$MAESTRO_ROOM_ID/ping</code></p> <pre><code>{\n    \"status\": \"ready\",\n    \"timestamp\": \"12312312313\"\n}\n</code></pre> <p>The status field can be:  - <code>ready</code>: the room is ready to accept players. - <code>occupied</code>: the room is occupied by one or more matches, and is not ready to accept more players. - <code>terminating</code>: the room is terminating, and will not accept any new players.</p>"},{"location":"tutorials/GettingStarted/#create-a-scheduler","title":"Create a scheduler","text":"<p>Use the command below to create a new scheduler, this will make a POST request for <code>/schedulers</code> endpoint.</p> <p>You need to change some parameters according to your game room image needs, for further details on all scheduler fields check the reference:</p> <ul> <li>image: your game room image.</li> <li>game: your game name (a same game can have multiple schedulers).</li> <li>name: your scheduler name (usually, the stack name).</li> <li>spec.command: any command that is required to run your game room.</li> <li>spec.environment: any environment variable required to run your game room.</li> <li>spec.ports: any port that must be exposed for clients to connect to the game room</li> </ul> <pre><code>curl --request POST \\\n  --url https://&lt;maestro-url&gt;/schedulers \\\n  --header 'Content-Type: application/json' \\\n  --header \"Authorization: Basic &lt;user:pass in base64&gt;\" \\\n  --data '{\n    \"name\": \"&lt;your-scheduler-name-here&gt;\",\n    \"game\": \"&lt;your-game-name-here&gt;\",\n    \"roomsReplicas\": 1,\n    \"portRange\": {\n            \"start\": 20000,\n            \"end\": 21000\n    },\n    \"maxSurge\": \"10%\",\n    \"spec\": {\n            \"terminationGracePeriod\": \"100s\",\n            \"containers\": [\n                {\n                    \"name\": \"game-container\",\n                    \"image\": \"&lt;your-game-image-here&gt;\",\n                    \"imagePullPolicy\": \"IfNotPresent\",\n                    \"command\": [\n                        &lt;required-commands-for-game-image&gt;\n                        \"sh example.sh\"\n                    ],\n                    \"environment\": [\n                        &lt;required-commands-for-game-image&gt;\n                        {\n                            \"name\": \"EXAMPLE_NAME\",\n                            \"value\": \"EXAMPLE_VALUE\"\n                        }\n                    ],\n                    \"requests\": {\n                        \"memory\": \"100Mi\",\n                        \"cpu\": \"100m\"\n                    },\n                    \"limits\": {\n                        \"memory\": \"200Mi\",\n                        \"cpu\": \"200m\"\n                    },\n                    \"ports\": [\n                        {\n                            \"name\": \"port-name\",\n                            \"protocol\": \"tcp\",\n                            \"port\": 12345\n                        }\n                    ]\n                }\n            ]\n    },\n    \"forwarders\": []\n}'\n</code></pre> <p>After running this command, a scheduler will be created with 1 game room replica.</p> <p>Then you can use the following command to get the scheduler details such as how many rooms are ready or occupied:</p> <pre><code>curl --location --request GET \"&lt;maestro-url&gt;/schedulers/info?game=&lt;your-game-name-here&gt;\" \\\n --header \"Accept: application/json\" \\\n --header \"Authorization: Basic &lt;user:pass in base64&gt;\"\n</code></pre>"}]}